{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf7ccb32",
      "metadata": {
        "id": "cf7ccb32"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-2/chatbot-external-memory.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239440-lesson-6-chatbot-w-summarizing-messages-and-external-memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6c7afe-1037-41ab-98e4-494692e47402",
      "metadata": {
        "id": "af6c7afe-1037-41ab-98e4-494692e47402"
      },
      "source": [
        "# Chatbot with message summarization & external DB memory\n",
        "\n",
        "## Review\n",
        "\n",
        "We've covered how to customize graph state schema and reducer.\n",
        "\n",
        "We've also shown a number of tricks for trimming or filtering messages in graph state.\n",
        "\n",
        "We've used these concepts in a Chatbot with memory that produces a running summary of the conversation.\n",
        "\n",
        "## Goals\n",
        "\n",
        "But, what if we want our Chatbot to have memory that persists indefinitely?\n",
        "\n",
        "Now, we'll introduce some more advanced checkpointers that support external databases.\n",
        "\n",
        "Here, we'll show how to use [Sqlite as a checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer), but other checkpointers, such as [Postgres](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/) are available!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "85ed78d9-6ca2-45ac-96a9-52e341ec519d",
      "metadata": {
        "id": "85ed78d9-6ca2-45ac-96a9-52e341ec519d"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph-checkpoint-sqlite langchain_core langgraph langchain_openai langchain_cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2e10c4d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "2e10c4d4",
        "outputId": "eeb92c95-e522-4298-b1ed-f68ed451fef3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3025672923.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{var}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3-3025672923.py\u001b[0m in \u001b[0;36m_set_env\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{var}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0m_set_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             )\n\u001b[0;32m-> 1159\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40d25c0-e9b5-4854-bf07-3cc3ff07122e",
      "metadata": {
        "id": "b40d25c0-e9b5-4854-bf07-3cc3ff07122e"
      },
      "source": [
        "## Sqlite\n",
        "\n",
        "A good starting point here is the [SqliteSaver checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer).\n",
        "\n",
        "Sqlite is a [small, fast, highly popular](https://x.com/karpathy/status/1819490455664685297) SQL database.\n",
        "\n",
        "If we supply `\":memory:\"` it creates an in-memory Sqlite database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fae15402-17ae-4e89-8ecf-4c89e08b22fe",
      "metadata": {
        "id": "fae15402-17ae-4e89-8ecf-4c89e08b22fe"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "# In memory\n",
        "conn = sqlite3.connect(\":memory:\", check_same_thread = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2bf53ec-6d4a-42ce-8183-344795eed403",
      "metadata": {
        "id": "c2bf53ec-6d4a-42ce-8183-344795eed403"
      },
      "source": [
        "But, if we supply a db path, then it will create a database for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "58339167-920c-4994-a0a7-0a9c5d4f7cf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58339167-920c-4994-a0a7-0a9c5d4f7cf7",
        "outputId": "074d8a9e-3dbf-42f2-dc84-0b77a7372855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-06 17:03:14--  https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/langchain-ai/langchain-academy/main/module-2/state_db/example.db [following]\n",
            "--2025-07-06 17:03:14--  https://raw.githubusercontent.com/langchain-ai/langchain-academy/main/module-2/state_db/example.db\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110592 (108K) [application/octet-stream]\n",
            "Saving to: ‘state_db/example.db’\n",
            "\n",
            "example.db          100%[===================>] 108.00K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-07-06 17:03:14 (4.10 MB/s) - ‘state_db/example.db’ saved [110592/110592]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# pull file if it doesn't exist and connect to local db\n",
        "!mkdir -p state_db && [ ! -f state_db/example.db ] && wget -P state_db https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db\n",
        "\n",
        "db_path = \"state_db/example.db\"\n",
        "conn = sqlite3.connect(db_path, check_same_thread=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3c7736b6-a750-48f8-a838-8e7616b12250",
      "metadata": {
        "id": "3c7736b6-a750-48f8-a838-8e7616b12250"
      },
      "outputs": [],
      "source": [
        "# Here is our checkpointer\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "memory = SqliteSaver(conn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d8cb629-213f-4b87-965e-19b812c42da1",
      "metadata": {
        "id": "9d8cb629-213f-4b87-965e-19b812c42da1"
      },
      "source": [
        "Let's re-define our chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc414e29-2078-41a0-887c-af1a6a3d72c0",
      "metadata": {
        "id": "dc414e29-2078-41a0-887c-af1a6a3d72c0"
      },
      "outputs": [],
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "\n",
        "from langgraph.graph import END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "model=ChatCohere(\n",
        "    model=\"command-r-08-2024\",\n",
        "    temperature=0.3,\n",
        "    cohere_api_key=\"ZQQlQaVY5GmRSrG2khFHLKwOgc5qZub4JEKEu0gP\"\n",
        ")\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State):\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State):\n",
        "\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State):\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c13c0b-a383-4f73-9cc1-63f0eed8f190",
      "metadata": {
        "id": "41c13c0b-a383-4f73-9cc1-63f0eed8f190"
      },
      "source": [
        "Now, we just re-compile with our sqlite checkpointer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e867fd95-91eb-4ce1-82fc-bb72d611a96d",
      "metadata": {
        "id": "e867fd95-91eb-4ce1-82fc-bb72d611a96d"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "# Set the entrypoint as conversation\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8769db99-3938-45e6-a594-56beb18d6c45",
      "metadata": {
        "id": "8769db99-3938-45e6-a594-56beb18d6c45"
      },
      "source": [
        "Now, we can invoke the graph several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4094a0-d240-4be8-903a-7d9f605bdc5c",
      "metadata": {
        "id": "0f4094a0-d240-4be8-903a-7d9f605bdc5c"
      },
      "outputs": [],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "input_message = HumanMessage(content=\"what's my name?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f3e842-4497-45e2-a924-69672a9bcb33",
      "metadata": {
        "id": "c0f3e842-4497-45e2-a924-69672a9bcb33"
      },
      "source": [
        "Let's confirm that our state is saved locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2ab158a-5a82-417a-8841-730a4cc18ea7",
      "metadata": {
        "id": "d2ab158a-5a82-417a-8841-730a4cc18ea7",
        "outputId": "e894553c-1c69-4508-fdff-1b77b074f64f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Lance\", id='d5bb4b3f-b1e9-4f61-8c75-7a7210b30253'), AIMessage(content=\"Hello again, Lance! It's great to hear from you. Since you like the 49ers, is there a particular player or moment in their history that stands out to you? Or perhaps you'd like to discuss their current season? Let me know!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 337, 'total_tokens': 387}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-dde04d51-d305-4a9e-8ad5-6bdf5583196e-0', usage_metadata={'input_tokens': 337, 'output_tokens': 50, 'total_tokens': 387}), HumanMessage(content=\"what's my name?\", id='d7530770-f130-4a05-a602-a96fd87859c6'), AIMessage(content='Your name is Lance! How can I assist you today? Would you like to talk more about the San Francisco 49ers or something else?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 243, 'total_tokens': 272}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-763b6387-c4c9-4658-9d01-7c018dde7a62-0', usage_metadata={'input_tokens': 243, 'output_tokens': 29, 'total_tokens': 272}), HumanMessage(content='i like the 49ers!', id='235dcec4-b656-4bee-b741-e330e1a026e2'), AIMessage(content=\"That's awesome, Lance! The San Francisco 49ers have a rich history and a passionate fan base. Is there a specific aspect of the team you'd like to discuss? For example, we could talk about:\\n\\n- Their legendary players like Joe Montana and Jerry Rice\\n- Memorable games and Super Bowl victories\\n- The current roster and season prospects\\n- Rivalries, like the one with the Seattle Seahawks\\n- Levi's Stadium and the fan experience\\n\\nLet me know what interests you!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 287, 'total_tokens': 385}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-729860b2-16c3-46ff-ad1e-0a5475565b20-0', usage_metadata={'input_tokens': 287, 'output_tokens': 98, 'total_tokens': 385})], 'summary': 'Here\\'s an extended summary of the conversation:\\n\\nLance introduced himself multiple times during the conversation, each time stating, \"Hi! I\\'m Lance.\" He expressed his fondness for the San Francisco 49ers football team. The AI assistant acknowledged Lance\\'s name each time and showed willingness to discuss the 49ers, offering to talk about various aspects of the team such as their history, current roster, memorable games, prospects for the upcoming season, rivalries, and their home stadium, Levi\\'s Stadium. Despite the AI\\'s attempts to engage in a more detailed discussion about the 49ers, Lance reintroduced himself again without directly responding to the AI\\'s questions or prompts about the team. The conversation remained brief and somewhat repetitive, focusing mainly on Lance\\'s introductions and his interest in the 49ers.'}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a36d-ca9c-6144-801b-6d0cf97adc73'}}, metadata={'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content=\"That's awesome, Lance! The San Francisco 49ers have a rich history and a passionate fan base. Is there a specific aspect of the team you'd like to discuss? For example, we could talk about:\\n\\n- Their legendary players like Joe Montana and Jerry Rice\\n- Memorable games and Super Bowl victories\\n- The current roster and season prospects\\n- Rivalries, like the one with the Seattle Seahawks\\n- Levi's Stadium and the fan experience\\n\\nLet me know what interests you!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 287, 'total_tokens': 385}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-729860b2-16c3-46ff-ad1e-0a5475565b20-0', usage_metadata={'input_tokens': 287, 'output_tokens': 98, 'total_tokens': 385})}}, 'step': 27, 'parents': {}}, created_at='2024-09-03T20:55:33.466540+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a36d-b8f3-6b40-801a-494776d2e9e0'}}, tasks=())"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e21152d-ed9c-408d-b7d5-f634c9ce81e2",
      "metadata": {
        "id": "1e21152d-ed9c-408d-b7d5-f634c9ce81e2"
      },
      "source": [
        "### Persisting state\n",
        "\n",
        "Using database like Sqlite means state is persisted!\n",
        "\n",
        "For example, we can re-start the notebook kernel and see that we can still load from Sqlite DB on disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a44dc5-be04-45fa-a6fc-27b0f8ee4678",
      "metadata": {
        "id": "b9a44dc5-be04-45fa-a6fc-27b0f8ee4678",
        "outputId": "94974ae4-45d0-447f-bc93-a697fa413b78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Lance\", id='d5bb4b3f-b1e9-4f61-8c75-7a7210b30253'), AIMessage(content=\"Hello again, Lance! It's great to hear from you. Since you like the 49ers, is there a particular player or moment in their history that stands out to you? Or perhaps you'd like to discuss their current season? Let me know!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 337, 'total_tokens': 387}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-dde04d51-d305-4a9e-8ad5-6bdf5583196e-0', usage_metadata={'input_tokens': 337, 'output_tokens': 50, 'total_tokens': 387}), HumanMessage(content=\"what's my name?\", id='d7530770-f130-4a05-a602-a96fd87859c6'), AIMessage(content='Your name is Lance! How can I assist you today? Would you like to talk more about the San Francisco 49ers or something else?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 243, 'total_tokens': 272}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-763b6387-c4c9-4658-9d01-7c018dde7a62-0', usage_metadata={'input_tokens': 243, 'output_tokens': 29, 'total_tokens': 272}), HumanMessage(content='i like the 49ers!', id='235dcec4-b656-4bee-b741-e330e1a026e2'), AIMessage(content=\"That's awesome, Lance! The San Francisco 49ers have a rich history and a passionate fan base. Is there a specific aspect of the team you'd like to discuss? For example, we could talk about:\\n\\n- Their legendary players like Joe Montana and Jerry Rice\\n- Memorable games and Super Bowl victories\\n- The current roster and season prospects\\n- Rivalries, like the one with the Seattle Seahawks\\n- Levi's Stadium and the fan experience\\n\\nLet me know what interests you!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 287, 'total_tokens': 385}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-729860b2-16c3-46ff-ad1e-0a5475565b20-0', usage_metadata={'input_tokens': 287, 'output_tokens': 98, 'total_tokens': 385})], 'summary': 'Here\\'s an extended summary of the conversation:\\n\\nLance introduced himself multiple times during the conversation, each time stating, \"Hi! I\\'m Lance.\" He expressed his fondness for the San Francisco 49ers football team. The AI assistant acknowledged Lance\\'s name each time and showed willingness to discuss the 49ers, offering to talk about various aspects of the team such as their history, current roster, memorable games, prospects for the upcoming season, rivalries, and their home stadium, Levi\\'s Stadium. Despite the AI\\'s attempts to engage in a more detailed discussion about the 49ers, Lance reintroduced himself again without directly responding to the AI\\'s questions or prompts about the team. The conversation remained brief and somewhat repetitive, focusing mainly on Lance\\'s introductions and his interest in the 49ers.'}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a36d-ca9c-6144-801b-6d0cf97adc73'}}, metadata={'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content=\"That's awesome, Lance! The San Francisco 49ers have a rich history and a passionate fan base. Is there a specific aspect of the team you'd like to discuss? For example, we could talk about:\\n\\n- Their legendary players like Joe Montana and Jerry Rice\\n- Memorable games and Super Bowl victories\\n- The current roster and season prospects\\n- Rivalries, like the one with the Seattle Seahawks\\n- Levi's Stadium and the fan experience\\n\\nLet me know what interests you!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 287, 'total_tokens': 385}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-729860b2-16c3-46ff-ad1e-0a5475565b20-0', usage_metadata={'input_tokens': 287, 'output_tokens': 98, 'total_tokens': 385})}}, 'step': 27, 'parents': {}}, created_at='2024-09-03T20:55:33.466540+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a36d-b8f3-6b40-801a-494776d2e9e0'}}, tasks=())"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8466e418-1a46-4cdb-a51a-6ae14281bb85",
      "metadata": {
        "id": "8466e418-1a46-4cdb-a51a-6ae14281bb85"
      },
      "source": [
        "## LangGraph Studio\n",
        "\n",
        "**⚠️ DISCLAIMER**\n",
        "\n",
        "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```\n",
        "\n",
        "You should see the following output:\n",
        "```\n",
        "- 🚀 API: http://127.0.0.1:2024\n",
        "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
        "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
        "```\n",
        "\n",
        "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4916d8b",
      "metadata": {
        "id": "c4916d8b"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}